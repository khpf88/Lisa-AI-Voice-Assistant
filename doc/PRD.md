# Product Requirements Document: Lisa AI Voice Assistant

## 1. Introduction

This document outlines the product requirements for "Lisa," a web-based AI Voice Assistant designed for conversational interaction. The application's primary goal is to be a lightweight, open-source, and free foundation for future projects, with an emphasis on running efficiently on CPU-only machines.

## 2. Functional Requirements

### 2.1. Core Functionality

*   **Voice Input & Voice Activity Detection (VAD):**
    *   The application shall continuously listen for voice input from the user via the microphone.
    *   **Dynamic VAD Thresholding:** The application now measures the ambient noise level of the user's environment and dynamically adjusts the VAD energy threshold. This results in more reliable speech detection in both quiet and noisy settings.
    *   **Server-side VAD shall be used to detect active speech.** The client will stream audio continuously to the server, which will perform VAD to identify utterances. Client-side audio capture now utilizes `AudioWorklet` for improved performance and modern browser compatibility. This approach is chosen for robustness and to centralize core logic on the server.
    *   The application shall not require explicit button presses for conversation initiation or continuation.
*   **Speech-to-Text (STT):**
    *   The application must accurately convert spoken voice input from the microphone into text using **Faster Whisper** on the server.
    *   The STT conversion should have low latency to support real-time interaction.
    *   The application shall intelligently combine partial speech inputs into full sentences for contextual understanding.
*   **Text-to-Speech (TTS):**
    *   The application must convert text generated by the language model into audible speech.
    *   **Enhanced Voice Selection:** Users can now select specific voices for both Kitten TTS and Kokoro TTS via environment variables, offering greater customization.
    *   **Improved Output Quality:** Unwanted LLM prompt tags are now filtered from the text before TTS synthesis, ensuring cleaner and more natural speech responses.
    *   **A true streaming pipeline shall be implemented.** The TTS engine will generate an in-memory audio stream (e.g., raw PCM) directly from the LLM's text output. This stream will be encoded on-the-fly and sent to the client, **avoiding the creation of any intermediate audio files** to minimize disk I/O and resource usage.
    *   The TTS output should be clear and natural-sounding, with efforts to minimize gaps between segments for smoother playback.
    *   **Aggressive sentence splitting** is used to reduce perceived latency. The system synthesizes smaller chunks of text (e.g., after commas) as they arrive from the LLM. To counteract potential choppiness from this approach, the client-side maintains a larger audio buffer.

*   **Language Model Integration:**
    *   **On-the-Fly Model Switching:** The application now uses different models based on the user's prompt. A smaller, faster model is used for general conversation, while a more capable model is used for tasks like coding. This optimizes performance and response quality.
    *   **Intelligent Response Summarization:** If a response from the LLM is too long, the application uses a secondary, quick LLM call to summarize it into a more concise, conversational reply before sending it to the TTS engine. The summarization LLM now has an increased token limit to ensure more complete summaries.
    *   The application will receive text input from the STT engine and generate responses using a Large Language Model.
    *   It currently focuses on a **local, CPU-optimized LLM** (e.g., LiquidAI/lfm2 via `llama.cpp`). Cloud-based LLM integration (e.g., Google Gemini API) is currently commented out.
    *   LLM responses will be streamed, allowing for sentence-by-sentence TTS synthesis and playback to reduce perceived latency.
    *   **Concise Responses:** LLM responses shall be consistently concise (1-2 sentences) and in plain text, suitable for natural conversation.
*   **Interruption Handling:**
    *   The application shall allow the user to interrupt Lisa's active TTS output via a dedicated on-screen button.
    *   Upon interruption, the application shall immediately stop its current TTS output and prepare to process new user input.

### 2.2. User Interface (UI)

*   **Main Screen:**
    *   The UI will have a clean and uncluttered layout, focusing on conversation display.
    *   Visual indicators (e.g., status text) shall show when the application is listening or speaking.
    *   **Dynamic Session Information Panel:** The UI shall display real-time information about the running session, including machine specifications (CPU, GPU, RAM), the specific STT, LLM, and TTS models currently in use, and the accumulated token count for the session.

## 3. Non-Functional Requirements

### 3.1. Performance

*   **Lightweight:** The application should be lightweight to ensure fast loading times and minimal resource consumption, specifically targeting low-specification, CPU-only hardware.
*   **Responsive:** The application must be fully responsive and provide an optimal user experience on various devices.
*   **Low Latency:** The perceived time from the end of user speech to the start of Lisa's audio response must be minimized.

### 3.2. Technology Stack

*   **Frontend:** Modern HTML, CSS, and JavaScript.
*   **Backend:** Python with FastAPI and WebSockets.
*   **VAD Engine:** A server-side library like `webrtcvad`.
*   **STT Engine:** **Faster Whisper** for local, CPU-optimized Speech-to-Text.
*   **TTS Engine:** `Kokoro FastAPI Wrapper` (external service) or `kittentts` (capable of in-memory, streaming TTS).
*   **Language Model:** Currently focused on local, CPU-optimized GGUF models run via `llama.cpp`. The specific model is configured in the `.env` file using the `LLM_MODEL_FILENAME` variable, allowing for easy swapping of models like TinyLlama, Gemma, and Phi-3.

### 3.3. Architecture

*   **Modularity:** The application should be designed in a modular way to allow for future expansion and easy maintenance.
*   **Server-Centric Logic:** The client handles audio capture (using `AudioWorklet`) and playback, while the server manages VAD, STT, LLM, and TTS processing. This centralizes the core logic on the server for better control and performance on resource-constrained clients.
*   **Asynchronous Processing:** All CPU-intensive tasks on the server (VAD, STT, LLM, TTS) are run in background threads or separate processes. For TTS, the system uses a truly dynamic `ProcessPoolExecutor` that allocates workers based on a declarative resource profile. Each TTS engine declares its own RAM and CPU requirements, and the system calculates the optimal number of workers by comparing this profile against live system resources (available RAM, CPU cores). This prevents resource exhaustion with heavy models while maximizing performance for lightweight ones. To further improve performance, if the calculated number of workers is 1, it is automatically increased to 2, ensuring a minimum of 2 cores are dedicated to TTS.
*   **Streaming Pipeline:** Data flows in a continuous stream from microphone to STT, to LLM, to TTS, and back to audio output, minimizing buffering and disk I/O.

## 4. Future Enhancements

*   **Voice Cloning:** Investigate and implement voice cloning capabilities for personalization, particularly for the elderly care companion app use case.
*   **Mobile Application:** Adapt the system to be the foundation for a mobile (iOS/Android) application.
*   **Advanced Conversation Management:** Implement features for managing conversation history, context, and user profiles.
*   **Deployment & Packaging:**
    *   Containerize the application (e.g., using Docker).
    *   Provide simplified deployment instructions.

### 4.5. Alternative TTS Engines

*   **Zyphra Zonos TTS:** This is a powerful, high-fidelity TTS engine with advanced voice cloning and expressive control features. While its large model size (1.6B parameters) and reliance on high-end GPUs make it unsuitable for the current project's CPU-only, low-latency requirements, it is a strong candidate for future iterations of this project if deployed on more powerful hardware with dedicated GPUs.